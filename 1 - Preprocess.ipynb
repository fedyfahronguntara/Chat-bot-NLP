{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install  necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install json\n",
    "# !pip install gensim\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word2vec model from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANDITYAARIFIANTO\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('word2vec.bin');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Conversation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('conversation3.json');\n",
    "data = json.load(file)\n",
    "cor=data[\"conversations\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a series of couple corpus as input data and target. <br>\n",
    "One corpus of conversation is the target for the previous conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "\n",
    "for i in range(len(cor)):\n",
    "    for j in range(len(cor[i])):\n",
    "        if j<len(cor[i])-1:\n",
    "            x.append(cor[i][j]);\n",
    "            y.append(cor[i][j+1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a conversation and it's target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to buy a fried chicken : Sure\n"
     ]
    }
   ],
   "source": [
    "idx = 21\n",
    "print(x[idx],':', y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize all input and target<br>\n",
    "create token for *'end-of-sentence'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_x=[]\n",
    "tok_y=[]\n",
    "for i in range(len(x)):\n",
    "    tok_x.append(nltk.word_tokenize(x[i].lower()))\n",
    "    tok_y.append(nltk.word_tokenize(y[i].lower()))\n",
    "\n",
    "sentence_end=np.ones((300,),dtype=np.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize all sentece token using gensim word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANDITYAARIFIANTO\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ANDITYAARIFIANTO\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "vec_x=[]\n",
    "for sent in tok_x:\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_x.append(sentvec)\n",
    "    \n",
    "vec_y=[]\n",
    "for sent in tok_y:\n",
    "    sentvec = [model[w] for w in sent if w in model.wv.vocab]\n",
    "    vec_y.append(sentvec)           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a conversation pair and it's token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to buy a fried chicken : Sure\n",
      "8 : 1\n"
     ]
    }
   ],
   "source": [
    "print(x[idx],':', y[idx])\n",
    "print(len(vec_x[idx]),':', len(vec_y[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the maximum length of a sentence vector is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok_sent in vec_x:\n",
    "    tok_sent[9:]=[]\n",
    "    tok_sent.append(sentence_end)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_x:\n",
    "    if len(tok_sent)<10:\n",
    "        for i in range(10-len(tok_sent)):\n",
    "            tok_sent.append(sentence_end)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok_sent in vec_y:\n",
    "    tok_sent[9:]=[]\n",
    "    tok_sent.append(sentence_end)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_y:\n",
    "    if len(tok_sent)<10:\n",
    "        for i in range(10-len(tok_sent)):\n",
    "            tok_sent.append(sentence_end)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a conversation pair and it's token lengths after limit it to 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to buy a fried chicken : Sure\n",
      "10 : 10\n"
     ]
    }
   ],
   "source": [
    "print(x[idx],':', y[idx])\n",
    "print(len(vec_x[idx]),':', len(vec_y[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the vectors to an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conversation.pickle','wb') as f:\n",
    "    pickle.dump([vec_x,vec_y],f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
